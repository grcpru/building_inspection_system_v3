"""
Building Inspection Data Processor - Database Integration
========================================================

Updated to work with the database schema while maintaining
all existing processing logic and adding comprehensive database integration.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import logging
from io import StringIO
import hashlib
import uuid

# Import database manager
try:
    from database.setup import DatabaseManager
    DATABASE_AVAILABLE = True
except ImportError:
    try:
        from core.database_manager import DatabaseManager
        DATABASE_AVAILABLE = True
    except ImportError:
        DATABASE_AVAILABLE = False
        logging.warning("Database manager not available - data will not be persisted")

# Set up logging
logger = logging.getLogger(__name__)


class InspectionDataProcessor:
    """Data processor with database integration for multi-role access"""
    
    def __init__(self, db_path: str = "building_inspection.db"):
        """Initialize the data processor with database support"""
        self.processed_data = None
        self.metrics = None
        self.building_info = {}
        
        # Initialize database manager if available
        if DATABASE_AVAILABLE:
            self.db_manager = DatabaseManager(db_path)
            logger.info("Database manager initialized")
        else:
            self.db_manager = None
            logger.warning("Database not available - data will only be stored in memory")
    
    def check_duplicate_file(self, file_bytes: bytes, filename: str) -> Optional[Dict]:
        """
        Check if this exact file was already processed.
        Returns dict with duplicate info if found, None otherwise.
        """
        if not self.db_manager:
            return None
        
        # Calculate file hash
        file_hash = hashlib.md5(file_bytes).hexdigest()
        
        try:
            conn = self.db_manager.connect()
            cursor = conn.cursor()
            
            # Check by hash first (most reliable)
            cursor.execute("""
                SELECT inspection_id, building_name, created_at, original_filename
                FROM inspector_csv_processing_log
                WHERE file_checksum = ?
                ORDER BY created_at DESC
                LIMIT 1
            """, (file_hash,))
            
            result = cursor.fetchone()
            
            if result:
                return {
                    'is_duplicate': True,
                    'inspection_id': result[0],
                    'building_name': result[1],
                    'processed_date': result[2],
                    'original_filename': result[3],
                    'file_hash': file_hash
                }
            
            # If no hash match, check by exact filename (less reliable but helpful)
            cursor.execute("""
                SELECT inspection_id, building_name, created_at, file_checksum
                FROM inspector_csv_processing_log
                WHERE original_filename = ?
                ORDER BY created_at DESC
                LIMIT 1
            """, (filename,))
            
            result = cursor.fetchone()
            
            if result and result[3] != file_hash:
                # Same filename but different content
                return {
                    'is_duplicate': False,
                    'warning': 'same_filename_different_content',
                    'previous_inspection_id': result[0],
                    'previous_date': result[2]
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Error checking duplicate: {e}")
            return None
        
    def process_inspection_data(self, df: pd.DataFrame, mapping: pd.DataFrame, 
                    building_info: Dict[str, str], 
                    inspector_name: str = "Inspector",
                    original_filename: str = None,
                    file_hash: str = None) -> Tuple[pd.DataFrame, Dict[str, Any], Optional[str]]:
        """Process inspection data with date and signoff tracking"""
        
        logger.info("Starting data processing with date and signoff tracking")
        
        try:
            df = df.copy()
            
            # STEP 1: Extract dates and signoffs FIRST
            logger.info("Extracting inspection dates...")
            df["InspectionDate"] = self._extract_unit_inspection_dates(df)
            
            logger.info("Extracting owner signoff timestamps...")
            df["OwnerSignoffTimestamp"] = self._extract_signoff_timestamp(df)
            
            # Log date range
            date_range = f"{df['InspectionDate'].min()} to {df['InspectionDate'].max()}"
            unique_dates = df['InspectionDate'].nunique()
            logger.info(f"Date range: {date_range} ({unique_dates} unique dates)")
            
            # Log signoff status
            signoff_count = df['OwnerSignoffTimestamp'].notna().sum()
            logger.info(f"Units with signoff: {signoff_count}/{len(df)}")
            
            # STEP 2: Extract unit numbers
            if "Lot Details_Lot Number" in df.columns and df["Lot Details_Lot Number"].notna().any():
                df["Unit"] = df["Lot Details_Lot Number"].astype(str).str.strip()
            elif "auditName" in df.columns:
                def extract_unit(audit_name):
                    if pd.isna(audit_name):
                        return "Unknown"
                    parts = str(audit_name).split("/")
                    if len(parts) >= 3:
                        candidate = parts[1].strip()
                        if len(candidate) <= 6 and any(ch.isdigit() for ch in candidate):
                            return candidate
                    return f"Unit_{hash(str(audit_name)) % 1000}"
                df["Unit"] = df["auditName"].apply(extract_unit)
            else:
                df["Unit"] = [f"Unit_{i}" for i in range(1, len(df) + 1)]

            # STEP 3: Derive unit type
            def derive_unit_type(row):
                unit_type = str(row.get("Pre-Settlement Inspection_Unit Type", "")).strip()
                if unit_type.lower() == "apartment":
                    return "Apartment"
                elif unit_type.lower() == "townhouse":
                    return "Townhouse"
                elif unit_type:
                    return unit_type
                return "Unknown Type"

            df["UnitType"] = df.apply(derive_unit_type, axis=1)

            # STEP 4: Get inspection columns
            inspection_cols = [c for c in df.columns 
                            if c.startswith("Pre-Settlement Inspection_") 
                            and not c.endswith("_notes")]
            
            if not inspection_cols:
                raise ValueError("No inspection columns found in CSV")

            # STEP 5: Melt data
            all_chunks = []
            chunk_size = 50
            
            for i in range(0, len(inspection_cols), chunk_size):
                chunk_cols = inspection_cols[i:i+chunk_size]
                
                chunk_df = df.melt(
                    id_vars=["Unit", "UnitType", "InspectionDate", "OwnerSignoffTimestamp"],
                    value_vars=chunk_cols,
                    var_name="InspectionItem",
                    value_name="Status"
                )
                all_chunks.append(chunk_df)

            long_df = pd.concat(all_chunks, ignore_index=True)

            # STEP 6: Split Room and Component
            parts = long_df["InspectionItem"].str.split("_", n=2, expand=True)
            if len(parts.columns) >= 3:
                long_df["Room"] = parts[1]
                long_df["Component"] = parts[2].str.replace(r"\.\d+$", "", regex=True)
                long_df["Component"] = long_df["Component"].apply(
                    lambda x: x.split("_")[-1] if isinstance(x, str) else x
                )
            else:
                long_df["Room"] = "General"
                long_df["Component"] = long_df["InspectionItem"].str.replace(
                    "Pre-Settlement Inspection_", ""
                )

            # STEP 7: Remove metadata
            metadata_rooms = ["Unit Type", "Building Type", "Townhouse Type", "Apartment Type"]
            metadata_components = ["Room Type"]
            long_df = long_df[~long_df["Room"].isin(metadata_rooms)]
            long_df = long_df[~long_df["Component"].isin(metadata_components)]

            # STEP 8: Classify status and urgency
            def classify_status(val):
                if pd.isna(val):
                    return "Blank"
                val_str = str(val).strip().lower()
                if val_str in ["✓", "✔", "ok", "pass", "passed", "good", "satisfactory"]:
                    return "OK"
                elif val_str in ["✗", "✘", "x", "fail", "failed", "not ok", "defect", "issue"]:
                    return "Not OK"
                elif val_str == "":
                    return "Blank"
                return "Not OK"

            def classify_urgency(val, component, room):
                if pd.isna(val):
                    return "Normal"
                val_str = str(val).strip().lower()
                component_str = str(component).lower()
                room_str = str(room).lower()
                
                urgent_keywords = ["urgent", "immediate", "safety", "hazard", "dangerous"]
                safety_components = ["fire", "smoke", "electrical", "gas", "water", "security"]
                
                if any(kw in val_str for kw in urgent_keywords):
                    return "Urgent"
                if any(sc in component_str for sc in safety_components):
                    return "High Priority"
                return "Normal"

            long_df["StatusClass"] = long_df["Status"].apply(classify_status)
            long_df["Urgency"] = long_df.apply(
                lambda row: classify_urgency(row["Status"], row["Component"], row["Room"]), 
                axis=1
            )

            # STEP 9: Merge with trade mapping
            merged = long_df.merge(mapping, on=["Room", "Component"], how="left")
            merged["Trade"] = merged["Trade"].fillna("Unknown Trade")
            
            mapping_success_rate = ((merged["Trade"] != "Unknown Trade").sum() / len(merged) * 100) if len(merged) > 0 else 0

            # STEP 10: Add planned completion
            def assign_planned_completion(urgency, inspection_date):
                base_date = pd.to_datetime(inspection_date)
                if urgency == "Urgent":
                    return base_date + timedelta(days=3)
                elif urgency == "High Priority":
                    return base_date + timedelta(days=7)
                return base_date + timedelta(days=14)

            merged["PlannedCompletion"] = merged.apply(
                lambda row: assign_planned_completion(row["Urgency"], row["InspectionDate"]), 
                axis=1
            )

            # STEP 11: Create final DataFrame
            final_df = merged[[
                "Unit", "UnitType", "InspectionDate", "OwnerSignoffTimestamp",
                "Room", "Component", "StatusClass", "Trade", "Urgency", "PlannedCompletion"
            ]]

            # STEP 12: Calculate metrics
            metrics = self._calculate_comprehensive_metrics(final_df, building_info, df)

            # STEP 13: Save to database
            inspection_id = None
            if self.db_manager:
                try:
                    final_df_for_db = final_df.copy()
                    final_df_for_db["PlannedCompletion"] = pd.to_datetime(
                        final_df_for_db["PlannedCompletion"]
                    ).dt.strftime('%Y-%m-%d')
                    
                    inspection_id = self.db_manager.save_inspector_data(
                        final_df_for_db, metrics, inspector_name, original_filename
                    )
                    
                    if inspection_id:
                        logger.info(f"✓ Saved to database: {inspection_id}")
                        
                except Exception as e:
                    logger.error(f"Database save failed: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            # STEP 14: Log CSV processing (AFTER all variables are defined)
            try:
                if inspection_id and file_hash:
                    self._log_csv_processing(
                        df, metrics, inspection_id, mapping_success_rate, 
                        inspector_name, original_filename, file_hash
                    )
            except Exception as log_error:
                logger.warning(f"CSV logging failed (non-critical): {log_error}")
            
            # STEP 15: Store results and return
            self.processed_data = final_df
            self.metrics = metrics
            
            logger.info(f"Processing complete: {len(final_df)} items, {metrics['total_defects']} defects")
            return final_df, metrics, inspection_id
            
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    def _log_csv_processing(self, original_df, metrics, inspection_id, mapping_success_rate, 
                       inspector_name, original_filename=None, file_hash=None):
        """Log CSV processing with file hash for duplicate detection"""
        
        print(f">>> _log_csv_processing CALLED - hash: {file_hash}, inspection_id: {inspection_id}")
        
        if not self.db_manager:
            print(">>> ERROR: No db_manager!")
            logger.error("Database manager not available")
            return
        
        try:
            conn = self.db_manager.connect()
            cursor = conn.cursor()
            
            log_id = str(uuid.uuid4())
            filename = original_filename or "uploaded_file.csv"
            
            print(f">>> Inserting: log_id={log_id[:8]}..., hash={file_hash[:8]}..., file={filename}")
            
            cursor.execute("""
                INSERT INTO inspector_csv_processing_log (
                    id, original_filename, file_checksum, file_size, inspector_id, 
                    building_name, total_rows, processed_rows, defects_found, 
                    mapping_success_rate, status, inspection_id, created_at, completed_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                log_id, 
                filename, 
                file_hash, 
                len(original_df) * 100,
                None,  # inspector_id
                metrics.get('building_name', 'Unknown Building'), 
                len(original_df), 
                len(original_df), 
                metrics.get('total_defects', 0), 
                mapping_success_rate,
                'completed', 
                inspection_id, 
                datetime.now(), 
                datetime.now()
            ))
            
            print(f">>> AFTER INSERT - rowcount: {cursor.rowcount}")
            conn.commit()
            print(f">>> AFTER COMMIT")
            
            rows_affected = cursor.rowcount
            conn.commit()
            
            print(f">>> SUCCESS: Inserted {rows_affected} row, committed to database")
            logger.info(f"CSV processing logged: hash={file_hash[:8]}...")
            
        except Exception as e:
            print(f">>> ERROR during INSERT: {e}")
            logger.error(f"Failed to log CSV processing: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
    def get_inspection_history(self, building_name: str = None, limit: int = 10) -> pd.DataFrame:
        """Get inspection history using database methods"""
        if not self.db_manager:
            logger.warning("Database not available for inspection history")
            return pd.DataFrame()
        
        try:
            return self.db_manager.get_inspector_inspections(limit)
        except Exception as e:
            logger.error(f"Error retrieving inspection history: {e}")
            return pd.DataFrame()
    
    def load_inspection_from_database(self, inspection_id: str) -> Tuple[pd.DataFrame, Dict]:
        """Load previously processed inspection from database with proper error handling"""
        if not self.db_manager:
            raise ValueError("Database not available")
        
        try:
            conn = self.db_manager.connect()
            
            # Get inspection metadata
            inspection_query = """
                SELECT i.*, b.name as building_name, b.address
                FROM inspector_inspections i
                JOIN inspector_buildings b ON i.building_id = b.id
                WHERE i.id = ?
            """
            inspection_df = pd.read_sql_query(inspection_query, conn, params=[inspection_id])
            
            if inspection_df.empty:
                raise ValueError(f"Inspection {inspection_id} not found")
            
            # Get inspection items
            items_query = """
                SELECT unit as Unit, unit_type as UnitType, 
                    inspection_date as InspectionDate,
                    owner_signoff_timestamp as OwnerSignoffTimestamp,
                    room as Room, component as Component, 
                    trade as Trade, status_class as StatusClass, 
                    urgency as Urgency, planned_completion as PlannedCompletion
                FROM inspector_inspection_items
                WHERE inspection_id = ?
            """
            items_df = pd.read_sql_query(items_query, conn, params=[inspection_id])
            
            # Parse timestamps with multiple format support
            if 'OwnerSignoffTimestamp' in items_df.columns:
                # Simply use errors='coerce' to handle all timestamp formats
                items_df['OwnerSignoffTimestamp'] = pd.to_datetime(
                    items_df['OwnerSignoffTimestamp'], 
                    errors='coerce'
                )
            
            # Parse dates
            if 'InspectionDate' in items_df.columns:
                items_df['InspectionDate'] = pd.to_datetime(items_df['InspectionDate'], errors='coerce')
            
            if 'PlannedCompletion' in items_df.columns:
                items_df['PlannedCompletion'] = pd.to_datetime(items_df['PlannedCompletion'], errors='coerce')
            
            inspection_row = inspection_df.iloc[0]
            
            # Recalculate metrics from actual data
            defects_only = items_df[items_df["StatusClass"] == "Not OK"]
            total_units = len(items_df["Unit"].unique()) if len(items_df) > 0 else 1
            
            # Calculate settlement readiness
            if len(defects_only) > 0:
                defects_per_unit = defects_only.groupby("Unit").size()
                ready_units_with_few_defects = (defects_per_unit <= 2).sum()
                
                units_with_defects = set(defects_per_unit.index)
                all_units = set(items_df["Unit"].dropna())
                units_with_no_defects = len(all_units - units_with_defects)
                ready_units_calculated = ready_units_with_few_defects + units_with_no_defects
                
                minor_work_units = ((defects_per_unit > 2) & (defects_per_unit <= 7)).sum()
                major_work_units = ((defects_per_unit > 7) & (defects_per_unit <= 15)).sum()
                extensive_work_units = (defects_per_unit > 15).sum()
            else:
                ready_units_calculated = total_units
                minor_work_units = 0
                major_work_units = 0
                extensive_work_units = 0
            
            ready_pct_calculated = (ready_units_calculated / total_units * 100) if total_units > 0 else 0
            minor_pct = (minor_work_units / total_units * 100) if total_units > 0 else 0
            major_pct = (major_work_units / total_units * 100) if total_units > 0 else 0
            extensive_pct = (extensive_work_units / total_units * 100) if total_units > 0 else 0
            
            # Calculate urgency metrics
            urgent_defects = defects_only[defects_only["Urgency"] == "Urgent"] if len(defects_only) > 0 else pd.DataFrame()
            high_priority_defects = defects_only[defects_only["Urgency"] == "High Priority"] if len(defects_only) > 0 else pd.DataFrame()
            
            # Calculate planned work timeframes
            planned_work_2weeks = pd.DataFrame()
            planned_work_month = pd.DataFrame()
            
            if len(defects_only) > 0 and 'PlannedCompletion' in defects_only.columns:
                try:
                    defects_only_copy = defects_only.copy()
                    defects_only_copy['PlannedCompletion'] = pd.to_datetime(defects_only_copy['PlannedCompletion'])
                    
                    next_two_weeks = datetime.now() + timedelta(days=14)
                    planned_work_2weeks = defects_only_copy[defects_only_copy["PlannedCompletion"] <= next_two_weeks]
                    
                    next_month = datetime.now() + timedelta(days=30)
                    planned_work_month = defects_only_copy[
                        (defects_only_copy["PlannedCompletion"] > next_two_weeks) & 
                        (defects_only_copy["PlannedCompletion"] <= next_month)
                    ]
                except Exception as e:
                    logger.warning(f"Could not calculate planned work: {e}")
            
            # Extract date information
            if 'InspectionDate' in items_df.columns:
                inspection_dates = items_df['InspectionDate'].dropna()
                if len(inspection_dates) > 0:
                    primary_date = inspection_dates.mode()[0] if len(inspection_dates.mode()) > 0 else inspection_dates.iloc[0]
                    min_date = inspection_dates.min()
                    max_date = inspection_dates.max()
                    
                    inspection_date_str = primary_date.strftime('%Y-%m-%d')
                    inspection_date_range = f"{min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}" if min_date != max_date else inspection_date_str
                    is_multi_day = min_date != max_date
                else:
                    inspection_date_str = str(inspection_row.get('inspection_date', '2025-01-01'))
                    inspection_date_range = inspection_date_str
                    is_multi_day = False
            else:
                inspection_date_str = str(inspection_row.get('inspection_date', '2025-01-01'))
                inspection_date_range = inspection_date_str
                is_multi_day = False
            
            # Build complete metrics dictionary
            metrics = {
                'building_name': str(inspection_row['building_name']),
                'address': str(inspection_row['address']),
                'inspection_date': inspection_date_str,
                'inspection_date_range': inspection_date_range,
                'is_multi_day_inspection': is_multi_day,
                'unit_types_str': ", ".join(sorted(items_df["UnitType"].astype(str).unique())) if len(items_df) > 0 else "Unknown",
                
                'total_units': total_units,
                'total_defects': len(defects_only),
                'total_inspections': len(items_df),
                'defect_rate': (len(defects_only) / len(items_df) * 100) if len(items_df) > 0 else 0.0,
                'avg_defects_per_unit': len(defects_only) / max(total_units, 1),
                
                'ready_units': ready_units_calculated,
                'ready_pct': ready_pct_calculated,
                'minor_work_units': minor_work_units,
                'major_work_units': major_work_units,
                'extensive_work_units': extensive_work_units,
                'minor_pct': minor_pct,
                'major_pct': major_pct,
                'extensive_pct': extensive_pct,
                
                'urgent_defects': len(urgent_defects),
                'high_priority_defects': len(high_priority_defects),
                'planned_work_2weeks': len(planned_work_2weeks),
                'planned_work_month': len(planned_work_month),
                
                'inspection_id': inspection_id
            }
            
            # Generate summary tables
            if len(defects_only) > 0:
                metrics['summary_trade'] = defects_only.groupby("Trade").size().reset_index(name="DefectCount").sort_values("DefectCount", ascending=False)
                metrics['summary_unit'] = defects_only.groupby("Unit").size().reset_index(name="DefectCount").sort_values("DefectCount", ascending=False)
                metrics['summary_room'] = defects_only.groupby("Room").size().reset_index(name="DefectCount").sort_values("DefectCount", ascending=False)
                metrics['urgent_defects_table'] = urgent_defects[["Unit", "Room", "Component", "Trade", "PlannedCompletion"]].copy() if len(urgent_defects) > 0 else pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "PlannedCompletion"])
                metrics['planned_work_2weeks_table'] = planned_work_2weeks[["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"]].copy() if len(planned_work_2weeks) > 0 else pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"])
                metrics['planned_work_month_table'] = planned_work_month[["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"]].copy() if len(planned_work_month) > 0 else pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"])
                metrics['component_details_summary'] = defects_only.groupby(["Trade", "Room", "Component"])["Unit"].apply(lambda s: ", ".join(sorted(s.astype(str).unique()))).reset_index().rename(columns={"Unit": "Units with Defects"})
            else:
                metrics['summary_trade'] = pd.DataFrame(columns=["Trade", "DefectCount"])
                metrics['summary_unit'] = pd.DataFrame(columns=["Unit", "DefectCount"])
                metrics['summary_room'] = pd.DataFrame(columns=["Room", "DefectCount"])
                metrics['urgent_defects_table'] = pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "PlannedCompletion"])
                metrics['planned_work_2weeks_table'] = pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"])
                metrics['planned_work_month_table'] = pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"])
                metrics['component_details_summary'] = pd.DataFrame(columns=["Trade", "Room", "Component", "Units with Defects"])
            
            self.processed_data = items_df
            self.metrics = metrics
            
            logger.info(f"Loaded inspection {inspection_id} from database")
            logger.info(f"Recalculated: Units={total_units}, Defects={len(defects_only)}, Ready={ready_units_calculated}")
            
            return items_df, metrics
            
        except Exception as e:
            logger.error(f"Error loading inspection from database: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def get_work_orders_for_builder(self, trade: str = None, status: str = None) -> pd.DataFrame:
        """Get work orders for Builder role using database"""
        if not self.db_manager:
            return pd.DataFrame()
        
        try:
            return self.db_manager.get_work_orders_for_builder(trade, status)
        except Exception as e:
            logger.error(f"Error retrieving work orders: {e}")
            return pd.DataFrame()
    
    def get_project_overview_for_developer(self) -> pd.DataFrame:
        """Get project overview for Developer role using database"""
        if not self.db_manager:
            return pd.DataFrame()
        
        try:
            return self.db_manager.get_project_overview_for_developer()
        except Exception as e:
            logger.error(f"Error retrieving project overview: {e}")
            return pd.DataFrame()
    
    def _calculate_comprehensive_metrics(self, final_df: pd.DataFrame, building_info: Dict, original_df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate metrics ensuring all values are proper Python types"""
        
        # Extract building name from auditName
        sample_audit = original_df.loc[0, "auditName"] if "auditName" in original_df.columns and len(original_df) > 0 else ""
        if sample_audit:
            audit_parts = str(sample_audit).split("/")
            extracted_building_name = audit_parts[2].strip() if len(audit_parts) >= 3 else building_info["name"]
        else:
            extracted_building_name = building_info["name"]
        
        # Extract date from final_df InspectionDate column
        if 'InspectionDate' in final_df.columns:
            inspection_dates = pd.to_datetime(final_df['InspectionDate'], errors='coerce').dropna()
            
            if len(inspection_dates) > 0:
                min_date = inspection_dates.min()
                max_date = inspection_dates.max()
                primary_date = inspection_dates.mode()[0] if len(inspection_dates.mode()) > 0 else min_date
                
                is_multi_day = (min_date != max_date)
                
                extracted_inspection_date = primary_date.strftime('%Y-%m-%d')
                inspection_date_range = f"{min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}"
            else:
                extracted_inspection_date = building_info.get("date", datetime.now().strftime("%Y-%m-%d"))
                inspection_date_range = extracted_inspection_date
                is_multi_day = False
        else:
            extracted_inspection_date = building_info.get("date", datetime.now().strftime("%Y-%m-%d"))
            inspection_date_range = extracted_inspection_date
            is_multi_day = False
        
        # Address information extraction
        location = ""
        area = ""
        region = ""

        # Extract building name from auditName
        if sample_audit:
            audit_parts = str(sample_audit).split("/")
            extracted_building_name = audit_parts[2].strip() if len(audit_parts) >= 3 else building_info["name"]
        else:
            extracted_building_name = building_info["name"]
        
        # Address information extraction
        location = ""
        area = ""
        region = ""
        
        if "Title Page_Site conducted_Location" in original_df.columns:
            location_series = original_df["Title Page_Site conducted_Location"].dropna()
            location = location_series.astype(str).str.strip().iloc[0] if len(location_series) > 0 else ""
        if "Title Page_Site conducted_Area" in original_df.columns:
            area_series = original_df["Title Page_Site conducted_Area"].dropna()
            area = area_series.astype(str).str.strip().iloc[0] if len(area_series) > 0 else ""
        if "Title Page_Site conducted_Region" in original_df.columns:
            region_series = original_df["Title Page_Site conducted_Region"].dropna()
            region = region_series.astype(str).str.strip().iloc[0] if len(region_series) > 0 else ""
        
        address_parts = [part for part in [location, area, region] if part]
        extracted_address = ", ".join(address_parts) if address_parts else building_info["address"]
        
        # Calculate settlement readiness using defects per unit
        defects_per_unit = final_df[final_df["StatusClass"] == "Not OK"].groupby("Unit").size()
        
        ready_units = (defects_per_unit <= 2).sum() if len(defects_per_unit) > 0 else 0
        minor_work_units = ((defects_per_unit > 2) & (defects_per_unit <= 7)).sum() if len(defects_per_unit) > 0 else 0
        major_work_units = ((defects_per_unit > 7) & (defects_per_unit <= 15)).sum() if len(defects_per_unit) > 0 else 0
        extensive_work_units = (defects_per_unit > 15).sum() if len(defects_per_unit) > 0 else 0
        
        # Add units with zero defects to ready category
        units_with_defects = set(defects_per_unit.index)
        all_units = set(final_df["Unit"].dropna())
        units_with_no_defects = len(all_units - units_with_defects)
        ready_units += units_with_no_defects
        
        total_units = final_df["Unit"].nunique()
        
        # Calculate basic metrics
        defects_only = final_df[final_df["StatusClass"] == "Not OK"]
        
        # Calculate urgency metrics
        urgent_defects = defects_only[defects_only["Urgency"] == "Urgent"]
        high_priority_defects = defects_only[defects_only["Urgency"] == "High Priority"]
        
        # Planned work calculations
        next_two_weeks = datetime.now() + timedelta(days=14)
        planned_work_2weeks = defects_only[defects_only["PlannedCompletion"] <= next_two_weeks]
        
        next_month = datetime.now() + timedelta(days=30)
        planned_work_month = defects_only[
            (defects_only["PlannedCompletion"] > next_two_weeks) & 
            (defects_only["PlannedCompletion"] <= next_month)
        ]
        
        # Helper function to ensure native Python types
        def ensure_python_type(value):
            """Convert numpy/pandas types to native Python types"""
            if hasattr(value, 'item'):
                return value.item()
            elif isinstance(value, (np.integer, np.floating)):
                return value.item()
            else:
                return value
        
        # Create metrics dictionary with proper Python types
        metrics = {
            "building_name": str(extracted_building_name),
            "address": str(extracted_address),
            "inspection_date": str(extracted_inspection_date),
            "inspection_date_range": str(inspection_date_range),  
            "is_multi_day_inspection": bool(is_multi_day),
            "unit_types_str": ", ".join(sorted(final_df["UnitType"].astype(str).unique())),
            "total_units": ensure_python_type(total_units),
            "total_inspections": ensure_python_type(len(final_df)),
            "total_defects": ensure_python_type(len(defects_only)),
            "defect_rate": ensure_python_type((len(defects_only) / len(final_df) * 100) if len(final_df) > 0 else 0.0),
            "avg_defects_per_unit": ensure_python_type((len(defects_only) / max(total_units, 1))),
            "ready_units": ensure_python_type(ready_units),
            "minor_work_units": ensure_python_type(minor_work_units),
            "major_work_units": ensure_python_type(major_work_units),
            "extensive_work_units": ensure_python_type(extensive_work_units),
            "ready_pct": ensure_python_type((ready_units / total_units * 100) if total_units > 0 else 0),
            "minor_pct": ensure_python_type((minor_work_units / total_units * 100) if total_units > 0 else 0),
            "major_pct": ensure_python_type((major_work_units / total_units * 100) if total_units > 0 else 0),
            "extensive_pct": ensure_python_type((extensive_work_units / total_units * 100) if total_units > 0 else 0),
            "urgent_defects": ensure_python_type(len(urgent_defects)),
            "high_priority_defects": ensure_python_type(len(high_priority_defects)),
            "planned_work_2weeks": ensure_python_type(len(planned_work_2weeks)),
            "planned_work_month": ensure_python_type(len(planned_work_month)),
            
            # Summary tables
            "summary_trade": defects_only.groupby("Trade").size().reset_index(name="DefectCount").sort_values("DefectCount", ascending=False) if len(defects_only) > 0 else pd.DataFrame(columns=["Trade", "DefectCount"]),
            "summary_unit": defects_only.groupby("Unit").size().reset_index(name="DefectCount").sort_values("DefectCount", ascending=False) if len(defects_only) > 0 else pd.DataFrame(columns=["Unit", "DefectCount"]),
            "summary_room": defects_only.groupby("Room").size().reset_index(name="DefectCount").sort_values("DefectCount", ascending=False) if len(defects_only) > 0 else pd.DataFrame(columns=["Room", "DefectCount"]),
            
            # Detailed tables for reporting
            "urgent_defects_table": urgent_defects[["Unit", "Room", "Component", "Trade", "PlannedCompletion"]].copy() if len(urgent_defects) > 0 else pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "PlannedCompletion"]),
            "planned_work_2weeks_table": planned_work_2weeks[["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"]].copy() if len(planned_work_2weeks) > 0 else pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"]),
            "planned_work_month_table": planned_work_month[["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"]].copy() if len(planned_work_month) > 0 else pd.DataFrame(columns=["Unit", "Room", "Component", "Trade", "Urgency", "PlannedCompletion"]),
            
            # Component details summary
            "component_details_summary": defects_only.groupby(["Trade", "Room", "Component"])["Unit"].apply(lambda s: ", ".join(sorted(s.astype(str).unique()))).reset_index().rename(columns={"Unit": "Units with Defects"}) if len(defects_only) > 0 else pd.DataFrame(columns=["Trade", "Room", "Component", "Units with Defects"])
        }
        
        logger.info(f"Metrics calculated: Building={extracted_building_name}, Units={total_units}, Defects={len(defects_only)}")
        return metrics

    def _extract_unit_inspection_dates(self, df: pd.DataFrame) -> pd.Series:
        """
        Extract inspection date for each row/unit.
        Priority: Title Page_Conducted on > auditName > current date
        """
        
        # Method 1: Title Page_Conducted on
        if "Title Page_Conducted on" in df.columns:
            dates = pd.to_datetime(df["Title Page_Conducted on"], errors='coerce')
            valid_count = dates.notna().sum()
            
            if valid_count > 0:
                logger.info(f"Found {valid_count} valid dates in 'Title Page_Conducted on'")
                # Fill missing with mode
                if dates.notna().any():
                    mode_date = dates.mode()[0] if len(dates.mode()) > 0 else dates.dropna().iloc[0]
                    dates = dates.fillna(mode_date)
                    return dates.dt.strftime('%Y-%m-%d')
        
        # Method 2: Extract from auditName
        if "auditName" in df.columns:
            def extract_date(audit_name):
                if pd.isna(audit_name):
                    return None
                parts = str(audit_name).split("/")
                if len(parts) >= 1:
                    try:
                        # Try DD/MM/YYYY format
                        date_obj = pd.to_datetime(parts[0].strip(), format='%d/%m/%Y', errors='coerce')
                        if pd.notna(date_obj):
                            return date_obj.strftime('%Y-%m-%d')
                    except:
                        pass
                return None
            
            dates = df["auditName"].apply(extract_date)
            valid_count = dates.notna().sum()
            
            if valid_count > 0:
                logger.info(f"Extracted {valid_count} dates from auditName")
                mode_date = dates.mode()[0] if len(dates.mode()) > 0 else dates.dropna().iloc[0]
                dates = dates.fillna(mode_date)
                return dates
        
        # Fallback
        logger.warning("Could not extract dates, using current date")
        return pd.Series([datetime.now().strftime('%Y-%m-%d')] * len(df))

    def _extract_signoff_timestamp(self, df: pd.DataFrame) -> pd.Series:
        """
        Extract owner/agent signature timestamp for each unit.
        Column: Sign Off_Owner/Agent Signature_timestamp
        """
        
        signoff_col = "Sign Off_Owner/Agent Signature_timestamp"
        
        if signoff_col in df.columns:
            timestamps = pd.to_datetime(df[signoff_col], errors='coerce')
            valid_count = timestamps.notna().sum()
            
            if valid_count > 0:
                logger.info(f"Found {valid_count} owner/agent sign-off timestamps")
            else:
                logger.info("Sign-off column exists but no valid timestamps")
            
            return timestamps
        else:
            logger.info("No sign-off timestamp column found")
            return pd.Series([None] * len(df))
    
def load_master_trade_mapping() -> pd.DataFrame:
    """Load master trade mapping with database fallback"""
    import os
    
    # Try to load from database first if available
    if DATABASE_AVAILABLE:
        try:
            db_manager = DatabaseManager()
            db_mapping = db_manager.get_trade_mapping()
            if len(db_mapping) > 0:
                logger.info("Loaded master trade mapping from database")
                return db_mapping
        except Exception as e:
            logger.warning(f"Could not load trade mapping from database: {e}")
    
    # Try to load from MasterTradeMapping.csv if it exists
    if os.path.exists("MasterTradeMapping.csv"):
        try:
            return pd.read_csv("MasterTradeMapping.csv")
        except Exception as e:
            logger.warning(f"Could not load MasterTradeMapping.csv: {e}")
    
    # Fallback to comprehensive embedded mapping
    mapping_data = """Room,Component,Trade
Apartment Entry Door,Door Handle,Doors
Apartment Entry Door,Door Locks and Keys,Doors
Apartment Entry Door,Paint,Painting
Apartment Entry Door,Door Frame,Carpentry & Joinery
Balcony,Balustrade,Carpentry & Joinery
Balcony,Drainage Point,Plumbing
Balcony,Flooring,Flooring - External
Balcony,Waterproofing,Waterproofing
Bathroom,Bathtub (if applicable),Plumbing
Bathroom,Ceiling,Painting
Bathroom,Exhaust Fan,Electrical
Bathroom,Tiles,Flooring - Tiles
Bathroom,Toilet,Plumbing
Bathroom,Shower,Plumbing
Bathroom,Vanity,Carpentry & Joinery
Bathroom,Tapware,Plumbing
Bathroom,Mirror,Glazing
Bathroom,Towel Rails,Accessories
Bathroom,Light Fixtures,Electrical
Bathroom,Power Outlets,Electrical
Bathroom,Walls,Painting
Bathroom,Flooring,Flooring - Tiles
Kitchen Area,Cabinets,Carpentry & Joinery
Kitchen Area,Kitchen Sink,Plumbing
Kitchen Area,Stovetop and Oven,Appliances
Kitchen Area,Rangehood,Appliances
Kitchen Area,Benchtop,Stone Work
Kitchen Area,Splashback,Flooring - Tiles
Kitchen Area,Dishwasher,Appliances
Kitchen Area,Light Fixtures,Electrical
Kitchen Area,Power Outlets,Electrical
Kitchen Area,Walls,Painting
Kitchen Area,Ceiling,Painting
Kitchen Area,Windows,Windows
Bedroom,Carpets,Flooring - Carpets
Bedroom,Windows,Windows
Bedroom,Light Fixtures,Electrical
Bedroom,Power Outlets,Electrical
Bedroom,Built-in Robes,Carpentry & Joinery
Bedroom,Ceiling,Painting
Bedroom,Walls,Painting
Bedroom,Flooring,Flooring
Bedroom,Built in Robes,Carpentry & Joinery
Living Room,Flooring,Flooring
Living Room,Windows,Windows
Living Room,Ceiling,Painting
Living Room,Walls,Painting
Living Room,Light Fixtures,Electrical
Living Room,Power Outlets,Electrical
Living Room,Air Conditioning,HVAC
Lounge,Flooring,Flooring
Lounge,Windows,Windows
Lounge,Ceiling,Painting
Lounge,Walls,Painting
Lounge,Light Fixtures,Electrical
Lounge,Power Outlets,Electrical
Lounge,Air Conditioning,HVAC
Dining Room,Flooring,Flooring
Dining Room,Windows,Windows
Dining Room,Ceiling,Painting
Dining Room,Walls,Painting
Dining Room,Light Fixtures,Electrical
Laundry,Washing Machine Taps,Plumbing
Laundry,Laundry Sink,Plumbing
Laundry,Cabinets,Carpentry & Joinery
Laundry,Flooring,Flooring
Laundry,Exhaust Fan,Electrical
Laundry,Light Fixtures,Electrical
Laundry,Walls,Painting
Laundry,Ceiling,Painting
Entry,Door,Doors
Entry,Intercom,Electrical
Entry,Flooring,Flooring
Entry,Light Fixtures,Electrical
Hallway,Light Switches,Electrical
Hallway,Smoke Detector,Fire Safety
Hallway,Flooring,Flooring
Hallway,Walls,Painting
Hallway,Ceiling,Painting
External,Hot Water System,Plumbing
External,Meter Box,Electrical
Car Space,Remote Control,Garage Doors
Car Space,Lighting,Electrical
Garage,Door,Garage Doors
Garage,Lighting,Electrical
Storage,Shelving,Carpentry & Joinery
General,Smoke Detector,Fire Safety
General,Air Conditioning,HVAC"""
    
    return pd.read_csv(StringIO(mapping_data))


# Utility function for integration with database
def process_inspection_csv(csv_path: str, trade_mapping: pd.DataFrame = None, 
                          building_info: Dict = None, inspector_name: str = "Inspector",
                          original_filename: str = None) -> Tuple[pd.DataFrame, Dict, str]:
    """
    Convenience function to process a CSV file directly with database storage
    
    Args:
        csv_path: Path to CSV file
        trade_mapping: Optional trade mapping DataFrame
        building_info: Optional building information
        inspector_name: Name of inspector
        original_filename: Original filename for tracking
        
    Returns:
        Tuple of (processed_data, metrics, inspection_id)
    """
    processor = InspectionDataProcessor()
    
    # Load CSV
    df = pd.read_csv(csv_path)
    
    # Use master mapping if none provided (with database fallback)
    if trade_mapping is None:
        trade_mapping = load_master_trade_mapping()
        logger.info("Using master trade mapping with database integration")
    
    # Default building info
    if building_info is None:
        building_info = {
            "name": "Professional Building Complex",
            "address": "123 Professional Street\nMelbourne, VIC 3000",
            "date": datetime.now().strftime("%Y-%m-%d")
        }
    
    # Use filename from path if not provided
    if original_filename is None:
        import os
        original_filename = os.path.basename(csv_path)
    
    return processor.process_inspection_data(df, trade_mapping, building_info, inspector_name, original_filename)


if __name__ == "__main__":
    print("Building Inspection Data Processor - Database Integration")
    print("Features:")
    print("- Complete working logic preserved from original")
    print("- Database integration with comprehensive schema")
    print("- Inspector data persisted for Builder and Developer")
    print("- Trade mapping database storage with fallbacks")
    print("- Inspection history and lookup with database methods")
    print("- CSV processing logging and tracking")
    print("- Work order and project overview integration")
    print("- Fixed binary data handling for proper display")
    print("Ready for Building Inspection System integration!")